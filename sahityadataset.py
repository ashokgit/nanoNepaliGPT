# -*- coding: utf-8 -*-
"""SahityaDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hvXf8xdGNjHcAC8Ds8I_v0D7n-q62UAp
"""

!pip install requests
!pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup
import csv
import xml.etree.ElementTree as ET
import re

def get_content(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    return response.content

def clean_html(html):
    soup = BeautifulSoup(html, 'html.parser')
    raw_text = soup.get_text()
    # Bag of words to remove
    remove_words = ["Share", "Tweet", "Subscribe", "SHARES"]

    # Remove unwanted words
    cleaned_text = ' '.join([word for word in raw_text.split() if word not in remove_words])

    # Reduce multiple newlines to one
    cleaned_text = re.sub('\n+', '\n', cleaned_text)
    cleaned_text = re.sub('\t+', '', cleaned_text)
    # Remove integer value at the start of the text
    cleaned_text = re.sub(r'^\d+\s*', '', cleaned_text)
    # Trim \n and \t characters from start and end
    cleaned_text = cleaned_text.strip('\n\t')
    return cleaned_text

def get_links_from_sitemap(sitemap_url):
    content = get_content(sitemap_url)
    root = ET.fromstring(content)
    links = [child[0].text for child in root]
    return links

def get_page_info(page_url):
    try:
        # Skip certain links
        if 'https://sahityapost.com/videos/' in page_url:
            return None
        content = get_content(page_url)
        soup = BeautifulSoup(content, 'html.parser')
        title = soup.find('title').text
        main_content = soup.find('div', {'class': 'main-content'}).prettify()
        if main_content:
            main_content_html = clean_html(main_content)
        else:
            main_content_html = ''

        # If there's no main content, return None
        if not main_content_html:
            return None

        return [page_url, title, main_content_html]
    except Exception as e:
        print(f"An error occurred while processing the page: {str(e)}")
        return None

def write_to_csv(data, filename):
    # Here we use 'a' to open the file for appending (not 'w', which would overwrite the file)
    with open(filename, 'a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(data)

import os

def main():
    sitemap_index_url = 'https://sahityapost.com/sitemap_index.xml'
    sitemap_urls = get_links_from_sitemap(sitemap_index_url)

    # Check if the data.csv file exists
    if os.path.exists('data.csv'):
        # Read existing URLs from data.csv
        with open('data.csv', 'r') as file:
            reader = csv.reader(file)
            existing_urls = {row[0] for row in reader}
    else:
        existing_urls = set()

    for sitemap_url in sitemap_urls:
        page_urls = get_links_from_sitemap(sitemap_url)
        for page_url in page_urls:
            # Skip the page if it already exists in the URL
            if page_url in existing_urls:
                continue

            if 'videos' in page_url:
                continue
            page_info = get_page_info(page_url)
            if page_info is not None:
                print(page_info)
                write_to_csv(page_info, 'data.csv')
